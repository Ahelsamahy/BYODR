import argparse
import fcntl
import json
import logging
import math
import multiprocessing
import sys
import time

import carla
import cv2
import numpy as np
import rospy
import v4l2
from std_msgs.msg import String as RosString

logger = logging.getLogger(__name__)
log_format = '%(levelname)s: %(filename)s %(funcName)s %(message)s'

quit_event = multiprocessing.Event()

CAMERA_SHAPE = (320, 480, 3)


class CarlaHandler(object):

    def __init__(self, **kwargs):
        self._world = kwargs.get('world')
        self._camera_callback = kwargs.get('camera_callback')
        self._actor = None
        self._image_shape = CAMERA_SHAPE
        self._sensors = []
        self._actor_lock = multiprocessing.Lock()
        self._actor_last_location = None
        self._actor_distance_traveled = 0.
        self._spawn_index = 0

    def _reset_agent_travel(self):
        logger.info("Actor distance traveled is {:8.3f}.".format(self._actor_distance_traveled))
        self._actor_distance_traveled = 0.
        self._actor_last_location = None

    def _destroy(self):
        if self._actor is not None and self._actor.is_alive:
            self._actor.destroy()
        for sensor in self._sensors:
            if sensor.is_alive:
                sensor.destroy()

    def _reset(self):
        logger.info('Resetting ...')
        self._destroy()
        #
        blueprint_library = self._world.get_blueprint_library()
        vehicle_bp = blueprint_library.find('vehicle.tesla.model3')
        spawn_points = self._world.get_map().get_spawn_points()
        spawn_idx = self._spawn_index + 1 if (self._spawn_index + 1) < len(spawn_points) else 0
        spawn_point = spawn_points[spawn_idx]
        logger.info("Spawn point is '{}'.".format(spawn_point))
        self._actor = self._world.spawn_actor(vehicle_bp, spawn_point)
        # Attach the camera's - defaults at https://carla.readthedocs.io/en/latest/cameras_and_sensors/.
        camera_bp = self._world.get_blueprint_library().find('sensor.camera.rgb')
        # Modify the attributes of the blueprint to set image resolution and field of view.
        im_height, im_width = self._image_shape[:2]
        camera_bp.set_attribute('image_size_x', '{}'.format(im_width))
        camera_bp.set_attribute('image_size_y', '{}'.format(im_height))
        # camera_bp.set_attribute('fov', '150')
        # Set the time in seconds between sensor captures
        camera_bp.set_attribute('sensor_tick', "{:2.2f}".format(1. / 100))
        # Provide the position of the sensor relative to the vehicle.
        # camera_transform = carla.Transform(carla.Location(x=0.8, z=1.7))
        camera_transform = carla.Transform(carla.Location(x=1.25, z=1.4))
        # Tell the world to spawn the sensor, don't forget to attach it to your vehicle actor.
        camera = self._world.spawn_actor(camera_bp, camera_transform, attach_to=self._actor)
        self._sensors.append(camera)
        # Subscribe to the sensor stream by providing a callback function, this function is
        # called each time a new image is generated by the sensor.
        camera.listen(lambda data: self._on_camera(data))
        self._reset_agent_travel()

    def _on_camera(self, data):
        img = np.frombuffer(data.raw_data, dtype=np.dtype("uint8"))
        _height, _width = self._image_shape[:2]
        img = np.reshape(img, (_height, _width, 4))  # To bgr_a format.
        img = img[:, :, :3]  # The image standard is hwc bgr.
        self._camera_callback(img)

    def _carla_vel(self):
        velocity = self._actor.get_velocity()
        return math.sqrt(velocity.x ** 2 + velocity.y ** 2 + velocity.z ** 2) / 1.6 / 3.6

    def _position(self):
        location = None if self._actor is None else self._actor.get_location()
        return (-1, -1) if location is None else (location.x, location.y)

    def _heading(self):
        return 0 if self._actor is None else self._actor.get_transform().rotation.yaw

    def _velocity(self):
        return 0 if self._actor is None else self._carla_vel()

    def state(self):
        x, y = self._position()
        return dict(x_coordinate=x, y_coordinate=y, heading=self._heading(), velocity=self._velocity())

    def start(self):
        self._reset()

    def quit(self):
        self._destroy()

    def tick(self, _):
        if self._actor is not None and self._actor.is_alive:
            with self._actor_lock:
                location = self._actor.get_location()
                if self._actor_last_location is not None:
                    _x, _y = self._actor_last_location
                    self._actor_distance_traveled += math.sqrt((location.x - _x) ** 2 + (location.y - _y) ** 2)
                self._actor_last_location = (location.x, location.y)

    def drive(self, cmd):
        if self._actor is not None:
            try:
                steering, throttle = cmd.get('steering'), cmd.get('throttle')
                control = carla.VehicleControl()
                control.steer = steering
                if throttle > 0:
                    control.throttle = throttle
                else:
                    control.brake = abs(throttle)
                self._actor.apply_control(control)
            except Exception as e:
                logger.error("{}".format(e))


def _ros_init():
    # Ros replaces the root logger - add a new handler after ros initialisation.
    rospy.init_node('carla', disable_signals=False, anonymous=True, log_level=rospy.INFO)
    console_handler = logging.StreamHandler(stream=sys.stdout)
    console_handler.setFormatter(logging.Formatter(log_format))
    logging.getLogger().addHandler(console_handler)
    logging.getLogger().setLevel(logging.INFO)
    rospy.on_shutdown(lambda: quit_event.set())


def hwc_bgr_to_yuyv(image):
    """
        1000 loops, best of 3: 1.42 ms per loop
    """
    imsize = image.shape[0] * image.shape[1] * 2
    buff = np.zeros(imsize, dtype=np.uint8)
    img = cv2.cvtColor(image, cv2.COLOR_BGR2YUV).ravel()
    ys = np.arange(0, img.shape[0], 3)
    vs = np.arange(1, img.shape[0], 6)
    us = np.arange(2, img.shape[0], 6)
    b_ys = np.arange(0, buff.shape[0], 2)
    b_us = np.arange(1, buff.shape[0], 4)
    b_vs = np.arange(3, buff.shape[0], 4)
    buff[b_ys] = img[ys]
    buff[b_us] = img[us]
    buff[b_vs] = img[vs]
    return buff


def main():
    parser = argparse.ArgumentParser(description='Carla vehicle client.')
    parser.add_argument('--remote', type=str, required=True, help='Carla server remote host:port')
    args = parser.parse_args()

    carla_host, carla_port = args.remote, 2000
    if ':' in carla_host:
        host, port = carla_host.split(':')
        carla_host, carla_port = host, int(port)
    carla_client = carla.Client(carla_host, carla_port)
    carla_client.set_timeout(2.)  # seconds

    # https://github.com/umlaeute/v4l2loopback/issues/32
    video_dev = open('/dev/video5', 'wrb', 0)  # 0 for unbuffered
    video_format = v4l2.v4l2_format()
    video_format.type = v4l2.V4L2_BUF_TYPE_VIDEO_OUTPUT
    video_format.fmt.pix.field = v4l2.V4L2_FIELD_NONE
    video_format.fmt.pix.pixelformat = v4l2.V4L2_PIX_FMT_YUYV
    height, width, channels = CAMERA_SHAPE
    video_format.fmt.pix.width = width
    video_format.fmt.pix.height = height
    video_format.fmt.pix.bytesperline = width * 2
    video_format.fmt.pix.sizeimage = width * height * 2
    video_format.fmt.pix.colorspace = v4l2.V4L2_COLORSPACE_JPEG
    _ioctl_result = fcntl.ioctl(video_dev, v4l2.VIDIOC_S_FMT, video_format)
    assert _ioctl_result == 0, "Failed to set v4l2 video format - ioctl result was '{}'.".format(_ioctl_result)

    _ros_init()
    vehicle_topic = rospy.Publisher('aav/vehicle/state/blob', RosString, queue_size=1)
    camera_topic = rospy.Publisher('aav/vehicle/camera/0', RosString, queue_size=1)

    # Publish the images.
    # http://api.zeromq.org/4-2:zmq-setsockopt
    # context = zmq.Context()
    # socket = context.socket(zmq.PUB)
    # socket.setsockopt(zmq.SNDHWM, 1)  # limit buffer size
    # socket.setsockopt(zmq.SNDTIMEO, 20)  # ms
    # socket.setsockopt(zmq.LINGER, 0)  # discard unsent messages on close
    # socket.bind('ipc:///tmp/byodr/zmq.sock')

    def _camera(_img):
        if not quit_event.is_set():  # and not socket.closed:
            _ts = time.time()
            camera_topic.publish(json.dumps(dict(time=_ts)))
            # socket.send_multipart(['topic/image',
            #                        json.dumps(dict(time=_ts, shape=_img.shape)),
            #                        np.ascontiguousarray(_img, dtype=np.uint8)], flags=0, copy=True, track=False)
            # noinspection PyTypeChecker
            video_dev.write(np.ascontiguousarray(hwc_bgr_to_yuyv(_img), dtype=np.uint8))

    world = carla_client.get_world()
    vehicle = CarlaHandler(world=world, camera_callback=_camera)
    vehicle.start()
    vehicle_tick = world.on_tick(lambda x: vehicle.tick(x))

    def _drive(data):
        vehicle.drive(json.loads(data.data))
        vehicle_topic.publish(json.dumps(vehicle.state()))

    rospy.Subscriber('aav/pilot/command/blob', RosString, _drive, queue_size=1)
    rospy.spin()

    # Done.
    # logger.info("Waiting on zmq to terminate.")
    # socket.close()
    # context.term()

    video_dev.close()

    logger.info("Waiting on carla to quit.")
    world.remove_on_tick(vehicle_tick)
    vehicle.quit()


if __name__ == "__main__":
    logging.basicConfig(format=log_format)
    logging.getLogger().setLevel(logging.DEBUG)
    main()
